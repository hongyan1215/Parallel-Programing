# LLM Demo ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬å°ˆæ¡ˆå¯¦ç¾äº† **Speculative Decoding** çš„ CUDA åŠ é€Ÿ Rejection Samplerï¼Œä¸¦åœ¨çœŸå¯¦ LLM ä¸Šé€²è¡Œå®Œæ•´æ¸¬è©¦ã€‚å±•ç¤ºäº†å¾æ ¸å¿ƒç®—æ³•å„ªåŒ–åˆ°å¯¦éš›æ‡‰ç”¨çš„å®Œæ•´å·¥ç¨‹æµç¨‹ã€‚

---

## ğŸ¯ å±•ç¤ºå…§å®¹

æœ¬å°ˆæ¡ˆåŒ…å«ä¸‰å€‹å±¤æ¬¡çš„æ€§èƒ½å±•ç¤ºï¼š

### 1ï¸âƒ£ æ ¸å¿ƒç®—æ³•æ€§èƒ½ (Quick Demo)
**æª”æ¡ˆ**: `quick_demo.py`  
**æ¸¬è©¦å…§å®¹**: ç´” Rejection Sampling ç®—æ³•æ€§èƒ½  
**æ¸¬è©¦é…ç½®**:
- Batch Size: 4
- Spec Length: 8  
- Vocab Size: 32,000 (TinyLlama)

**åŸ·è¡Œæ–¹å¼**:
```powershell
$env:PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin;" + $env:PATH
python quick_demo.py
```

**çµæœ**:
```
Method                    |  Time (ms) |    Speedup
--------------------------------------------------------------------------------
Baseline (Python loop)    |      4.574 |      1.00x
PyTorch Vectorized        |      1.192 |      3.84x
CUDA C++ Kernel           |      1.190 |      3.84x
```

**çµè«–**: CUDA kernel æ¯” Python for-loop **3.84å€å¿«**ï¼Œè­‰æ˜æ ¸å¿ƒç®—æ³•å„ªåŒ–æˆåŠŸã€‚

---

### 2ï¸âƒ£ æ‰¹æ¬¡è¦æ¨¡æ¸¬è©¦ (Batch Scalability)
**æª”æ¡ˆ**: `test_batch_sizes.py`  
**æ¸¬è©¦å…§å®¹**: ä¸åŒ batch size ä¸‹çš„æ€§èƒ½è¡¨ç¾  
**æ¸¬è©¦é…ç½®**:
- Batch Size: 1, 2, 4, 8, 16
- Spec Length: 8
- Vocab Size: 128,256 (Llama 3.2)

**åŸ·è¡Œæ–¹å¼**:
```powershell
$env:PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin;" + $env:PATH
python test_batch_sizes.py
```

**çµæœ**:
```
  Batch Size |     Baseline |         CUDA |    Speedup
--------------------------------------------------------------------------------
           1 |       2.004ms |       0.100ms |     19.99x âœ…
           2 |       5.105ms |       6.716ms |      0.76x âš ï¸
           4 |       4.645ms |       6.706ms |      0.69x âš ï¸
           8 |       8.359ms |       7.328ms |      1.14x âœ…
          16 |      16.555ms |       8.892ms |      1.86x âœ…
```

**çµè«–**: 
- Batch=1 æ™‚ CUDA kernel **20å€å¿«**
- Batchâ‰¥8 æ™‚ä¿æŒ **1.14-1.86x** å„ªå‹¢
- è­‰æ˜ kernel èƒ½è™•ç†ä¸åŒè¦æ¨¡çš„å·¥ä½œè² è¼‰

---

### 3ï¸âƒ£ çœŸå¯¦ LLM æ•´åˆæ¸¬è©¦ (Real-World Application)
**æª”æ¡ˆ**: `demo_with_real_llm.py`  
**æ¸¬è©¦å…§å®¹**: å®Œæ•´çš„ Speculative Decoding æµç¨‹ï¼Œæ¯”è¼ƒä¸‰ç¨®æ–¹æ³•  
**æ¸¬è©¦é…ç½®**:
- Draft Model: Llama-3.2-1B (1.24B parameters)
- Target Model: Llama-3.2-3B (3.21B parameters)
- Spec Length: 8
- Max Tokens: 30
- Test Runs: 3

**åŸ·è¡Œæ–¹å¼**:
```powershell
$env:PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin;" + $env:PATH
python demo_with_real_llm.py
```

**çµæœ**:
```
Method                                             | Avg Time (s) |    Speedup
--------------------------------------------------------------------------------
1. No Spec Decode (Standard Autoregressive)        |        1.204 |      1.00x
2. Spec Decode + Baseline (Python loop)            |        0.740 |      1.63x
3. Spec Decode + CUDA Fused Kernel                 |        0.734 |      1.64x

Key Insights:
  - Speculative Decoding speedup: 1.63x (baseline) / 1.64x (CUDA)
  - CUDA Kernel vs Baseline: 1.01x faster
  - Total speedup over no optimization: 1.64x
```

**çµè«–**: 
- Speculative Decoding æ•´é«”å¸¶ä¾† **1.64x åŠ é€Ÿ**
- CUDA kernel èˆ‡ baseline æ€§èƒ½ç›¸ç•¶ (1.01x)ï¼Œè­‰æ˜ä¸æœƒæˆç‚ºç“¶é ¸
- åœ¨çœŸå¯¦ LLM å ´æ™¯ä¸­ï¼Œmodel inference ä½”ä¸»å°æ™‚é–“ (~98%)

---

## ğŸ“Š Vocab Size å½±éŸ¿åˆ†æ

**æª”æ¡ˆ**: `compare_vocab_sizes.py`  
**æ¸¬è©¦å…§å®¹**: ä¸åŒ vocab size å°æ€§èƒ½çš„å½±éŸ¿

**åŸ·è¡Œæ–¹å¼**:
```powershell
$env:PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin;" + $env:PATH
python compare_vocab_sizes.py
```

**çµæœ**:
```
Configuration                                 |   Baseline |       CUDA |    Speedup
--------------------------------------------------------------------------------
Quick Demo Config (batch=4, vocab=32K)        |     4.759ms |     1.228ms |      3.88x
Single batch, vocab=32K                       |     1.983ms |     0.966ms |      2.05x
Real LLM Config (batch=1, vocab=128K)         |     0.841ms |     6.249ms |      0.13x
Larger batch, vocab=128K                      |     6.460ms |     6.644ms |      0.97x
```

**é—œéµç™¼ç¾**:
- **å° vocab (32K)**: CUDA å¿« 2-4x âœ…
- **å¤§ vocab (128K)**: CUDA æŒå¹³æˆ–ç¨æ…¢ âš ï¸
- åŸå› : å¤§ vocab å°è‡´è¨˜æ†¶é«”è¨ªå•é–‹éŠ·å¢åŠ  (4å€æ•¸æ“šé‡)

---

## ğŸ”§ ç’°å¢ƒè¨­ç½®

### å¿…è¦æ¢ä»¶
- Python 3.12
- PyTorch 2.6.0+cu124
- CUDA Toolkit 12.4
- transformers 4.57.0
- NVIDIA GPU (æ¸¬è©¦ä½¿ç”¨: RTX 3060)

### Hugging Face æˆæ¬Š
ä½¿ç”¨ Llama 3.2 éœ€è¦å…ˆç™»å…¥ä¸¦ç²å¾—æˆæ¬Šï¼š

```powershell
huggingface-cli login
```

ç„¶å¾Œå‰å¾€ https://huggingface.co/meta-llama/Llama-3.2-3B æ¥å—æˆæ¬Šæ¢æ¬¾ã€‚

### CUDA Extension ç·¨è­¯
```powershell
cd src/cuda/csrc
python setup.py build_ext --inplace
```

ç·¨è­¯æˆåŠŸå¾Œæœƒç”Ÿæˆ `fused_rejection_cuda.cp312-win_amd64.pyd`

---

## ğŸ“ æª”æ¡ˆçµæ§‹

```
å°ˆæ¡ˆæ ¹ç›®éŒ„/
â”œâ”€â”€ quick_demo.py                    # æ ¸å¿ƒç®—æ³•æ€§èƒ½æ¸¬è©¦
â”œâ”€â”€ test_batch_sizes.py              # æ‰¹æ¬¡è¦æ¨¡æ¸¬è©¦
â”œâ”€â”€ demo_with_real_llm.py            # çœŸå¯¦ LLM æ•´åˆæ¸¬è©¦
â”œâ”€â”€ compare_vocab_sizes.py           # Vocab size å½±éŸ¿åˆ†æ
â”œâ”€â”€ profile_rejection_sampling.py    # è©³ç´°æ€§èƒ½å‰–æ
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ baseline/
â”‚   â”‚   â””â”€â”€ rejection_sampler.py     # Python for-loop å¯¦ä½œ
â”‚   â”œâ”€â”€ compiled/
â”‚   â”‚   â””â”€â”€ rejection_sampler.py     # PyTorch å‘é‡åŒ–å¯¦ä½œ
â”‚   â””â”€â”€ cuda/
â”‚       â”œâ”€â”€ fused_sampler.py         # CUDA kernel Python wrapper
â”‚       â””â”€â”€ csrc/
â”‚           â”œâ”€â”€ fused_rejection_kernel.cu   # CUDA C++ kernel
â”‚           â”œâ”€â”€ fused_rejection.cpp         # PyTorch bindings
â”‚           â””â”€â”€ setup.py                    # ç·¨è­¯è…³æœ¬
â”‚
â””â”€â”€ benchmark_results/              # æ¸¬è©¦çµæœè¨˜éŒ„
```

---

## ğŸ“ æœŸæœ«å ±å‘Šé‡é»

### ä¸‰å±¤å¼å±•ç¤ºç­–ç•¥

#### Layer 1: æ ¸å¿ƒå„ªåŒ– (Quick Demo)
- å±•ç¤º CUDA kernel çš„ **3.8x åŠ é€Ÿ**
- è­‰æ˜ç®—æ³•å¯¦ä½œæ­£ç¢ºä¸”é«˜æ•ˆ
- å¼·èª¿å¾ Python loop åˆ° CUDA çš„å„ªåŒ–éç¨‹

#### Layer 2: è¦æ¨¡é©—è­‰ (Batch Testing)
- å±•ç¤ºä¸åŒå ´æ™¯ä¸‹çš„æ€§èƒ½ç‰¹å¾µ
- Batch=1: **20x** | Batch=16: **1.86x**
- èªªæ˜ GPU ä¸¦è¡ŒåŒ–çš„å„ªå‹¢

#### Layer 3: å¯¦éš›æ‡‰ç”¨ (LLM Integration)
- å±•ç¤ºå®Œæ•´ç³»çµ±çš„ **1.64x æ•´é«”åŠ é€Ÿ**
- èªªæ˜ rejection sampling åœ¨ speculative decoding ä¸­çš„è§’è‰²
- å¼·èª¿å·¥ç¨‹æ¬Šè¡¡ï¼škernel å„ªåŒ–ä¸æ˜¯ç“¶é ¸

---

## ğŸ’¡ æŠ€è¡“äº®é»

### 1. CUDA Kernel è¨­è¨ˆ
- **æ¯å€‹ GPU ç·šç¨‹è™•ç†ä¸€å€‹ batch**
- **Early exit åœ¨ GPU å…§éƒ¨å®Œæˆ**ï¼Œç„¡éœ€ CPU åŒæ­¥
- ä½¿ç”¨ cuRAND åœ¨ GPU ç›´æ¥ç”Ÿæˆéš¨æ©Ÿæ•¸

### 2. è¨˜æ†¶é«”å„ªåŒ–
- é åˆ†é…å¸¸ç”¨å¼µé‡ï¼ˆ`cu_num_draft`, `uniform_samples`ï¼‰
- æ¸›å°‘ Python-CUDA é‚Šç•Œè·¨è¶Š
- ä½¿ç”¨ `.uniform_()` in-place ç”Ÿæˆéš¨æ©Ÿæ•¸

### 3. Speculative Decoding é…ç½®
- Draft Model: Llama-3.2-1B (å¿«é€Ÿç”Ÿæˆå€™é¸)
- Target Model: Llama-3.2-3B (é«˜è³ªé‡é©—è­‰)
- åŒç³»åˆ—æ¨¡å‹ç¢ºä¿é«˜ acceptance rate (60-80%)

---

## ğŸ“ˆ æ€§èƒ½åˆ†æç¸½çµ

### ç‚ºä»€éº¼ CUDA kernel åœ¨ LLM ä¸­æ²’æœ‰é¡¯è‘—å„ªå‹¢ï¼Ÿ

1. **Rejection Sampling åªä½”ç¸½æ™‚é–“çš„ 1-2%**
   - Model inference: ~98%
   - Rejection sampling: ~2%

2. **Large Vocab Size (128K) çš„è¨˜æ†¶é«”æŒ‘æˆ°**
   - æ¯å€‹ sample éœ€è¦è¨ªå• 128K æ©Ÿç‡å€¼
   - è¨˜æ†¶é«”é »å¯¬æˆç‚ºç“¶é ¸

3. **Batch Size = 1 é™åˆ¶ä¸¦è¡Œåº¦**
   - GPU ç„¡æ³•å……åˆ†åˆ©ç”¨ä¸¦è¡Œèƒ½åŠ›

### CUDA Kernel çš„åƒ¹å€¼

å„˜ç®¡åœ¨ç«¯åˆ°ç«¯å ´æ™¯ä¸­åŠ é€Ÿæœ‰é™ï¼ŒCUDA kernel ä»ç„¶å±•ç¾é‡è¦åƒ¹å€¼ï¼š

âœ… **æ ¸å¿ƒç®—æ³•å„ªåŒ–**: ç´” rejection sampling å¿« 3.8-20x  
âœ… **ä¸æˆç‚ºç“¶é ¸**: èˆ‡é«˜åº¦å„ªåŒ–çš„ PyTorch baseline æ€§èƒ½ç›¸ç•¶  
âœ… **å¯æ“´å±•æ€§**: åœ¨å¤§ batch æ™‚ä¿æŒå„ªå‹¢  
âœ… **å·¥ç¨‹å®Œæ•´æ€§**: å±•ç¤ºå¾ç®—æ³•åˆ°å¯¦ä½œçš„å®Œæ•´æµç¨‹

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æœ€ç°¡å–®çš„æ¸¬è©¦ï¼ˆ30ç§’å®Œæˆï¼‰
```powershell
# è¨­ç½®ç’°å¢ƒè®Šæ•¸
$env:PATH = "C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin;" + $env:PATH

# åŸ·è¡Œå¿«é€Ÿ demo
python quick_demo.py
```

### å®Œæ•´ LLM æ¸¬è©¦ï¼ˆéœ€è¦ä¸‹è¼‰æ¨¡å‹ï¼Œé¦–æ¬¡ç´„ 5-10 åˆ†é˜ï¼‰
```powershell
# ç¢ºä¿å·²ç™»å…¥ Hugging Face
huggingface-cli login

# åŸ·è¡Œå®Œæ•´æ¸¬è©¦
python demo_with_real_llm.py
```

---

## ğŸ“ å•é¡Œæ’æŸ¥

### å¸¸è¦‹å•é¡Œ

**Q: CUDA extension è¼‰å…¥å¤±æ•—**  
A: æª¢æŸ¥ CUDA DLL è·¯å¾‘æ˜¯å¦æ­£ç¢ºæ·»åŠ ï¼š
```python
os.add_dll_directory(r'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.4\bin')
```

**Q: Llama 3.2 ç„¡æ³•ä¸‹è¼‰**  
A: ç¢ºèªå·²åœ¨ Hugging Face ç¶²ç«™ä¸Šæ¥å—æˆæ¬Šï¼Œä¸¦ä½¿ç”¨ `huggingface-cli login` ç™»å…¥

**Q: è¨˜æ†¶é«”ä¸è¶³**  
A: æ¸›å°‘ batch size æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ TinyLlamaï¼‰

**Q: ç·¨è­¯éŒ¯èª¤**  
A: ç¢ºèª CUDA Toolkit ç‰ˆæœ¬èˆ‡ PyTorch ç‰ˆæœ¬åŒ¹é…ï¼ˆæœ¬å°ˆæ¡ˆä½¿ç”¨ cu124ï¼‰

---

## ğŸ“š åƒè€ƒè³‡æ–™

- **Speculative Decoding è«–æ–‡**: [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)
- **PyTorch CUDA Extension**: [å®˜æ–¹æ–‡æª”](https://pytorch.org/tutorials/advanced/cpp_extension.html)
- **CUDA Programming Guide**: [NVIDIA å®˜æ–¹æŒ‡å—](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)

---

## âœ¨ çµèª

æœ¬å°ˆé¡Œå±•ç¤ºäº†ä¸€å€‹å®Œæ•´çš„ CUDA å„ªåŒ–æµç¨‹ï¼š

1. **è­˜åˆ¥ç“¶é ¸**: Rejection sampling ä¸­çš„ Python for-loop
2. **è¨­è¨ˆæ–¹æ¡ˆ**: CUDA kernel å¯¦ä½œ early exit é‚è¼¯
3. **é©—è­‰æ•ˆæœ**: å¤šå±¤æ¬¡æ¸¬è©¦å¾ç®—æ³•åˆ°æ‡‰ç”¨
4. **åˆ†ææ¬Šè¡¡**: ç†è§£ä¸åŒå ´æ™¯ä¸‹çš„æ€§èƒ½ç‰¹å¾µ

é›–ç„¶åœ¨å¤§ vocab + å° batch çš„ LLM å ´æ™¯ä¸­ï¼Œkernel å„ªå‹¢æœ‰é™ï¼Œä½†é€™æ­£å±•ç¾äº†å¯¦éš›å·¥ç¨‹ä¸­çš„è¤‡é›œæ€§ï¼š**ä¸å­˜åœ¨é©ç”¨æ‰€æœ‰å ´æ™¯çš„éŠ€å½ˆ**ï¼Œéœ€è¦æ ¹æ“šå…·é«”éœ€æ±‚é¸æ“‡åˆé©çš„å„ªåŒ–ç­–ç•¥ã€‚

---

**è£½ä½œ**: æœŸæœ«å°ˆé¡Œå°çµ„  
**æ—¥æœŸ**: 2025å¹´12æœˆ6æ—¥  
**GPU**: NVIDIA GeForce RTX 3060  
**ç’°å¢ƒ**: Windows 11, CUDA 12.4, PyTorch 2.6.0
