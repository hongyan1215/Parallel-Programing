# ğŸš€ Speculative Decoding Fused Sampling Kernel å¯¦ä½œæŒ‡å—

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¿°

æœ¬å°ˆæ¡ˆç›®æ¨™æ˜¯å¯¦ä½œä¸€å€‹ **Fused CUDA Kernel** ä¾†å„ªåŒ– Speculative Decoding çš„ Rejection Sampling éšæ®µï¼Œå°‡åŸæœ¬ O(K) çš„ kernel launch overhead é™ä½ç‚º O(1)ã€‚

### æ ¸å¿ƒå•é¡Œ

åœ¨ Speculative Decoding ä¸­ï¼Œdraft model ç”¢ç”Ÿ K å€‹å€™é¸ tokens å¾Œï¼Œéœ€è¦ç”¨ target model é©—è­‰ä¸¦æ±ºå®šæ¥å—/æ‹’çµ•ã€‚ç›®å‰çš„å¯¦ä½œå­˜åœ¨ä»¥ä¸‹ç“¶é ¸ï¼š

```
å•é¡Œï¼šPython for loop å°è‡´ O(K) æ¬¡ kernel launch
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Launch 1â”‚â”€â”€â–¶â”‚ Launch 2â”‚â”€â”€â–¶â”‚ Launch Kâ”‚  â† æ¯æ¬¡ launch éƒ½æœ‰ overhead
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“             â†“             â†“
   é©—è­‰ tâ‚       é©—è­‰ tâ‚‚       é©—è­‰ tâ‚–
```

### è§£æ±ºæ–¹æ¡ˆ

```
ç›®æ¨™ï¼šå–®ä¸€ Fused Kernelï¼ŒO(1) launch
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Fused CUDA Kernel            â”‚  â† åªæœ‰ 1 æ¬¡ launch overhead
â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”   â”‚
â”‚  â”‚ tâ‚ â”‚â†’â”‚ tâ‚‚ â”‚â†’â”‚ tâ‚ƒ â”‚â†’...â†’â”‚ tâ‚– â”‚   â”‚  â† æ‰€æœ‰é©—è­‰åœ¨ GPU å…§å®Œæˆ
â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š ä¸‰ç´šå¯¦ä½œæ¶æ§‹

| Level | åç¨± | å¯¦ä½œæ–¹å¼ | é æœŸæ•ˆèƒ½ | ç”¨é€” |
|-------|------|----------|----------|------|
| **L1** | Baseline | PyTorch for loop | O(K) | æ­£ç¢ºæ€§æ¨™æº– |
| **L2** | Competitor | `@torch.compile` | O(K) | å±•ç¤ºç·¨è­¯å™¨é™åˆ¶ |
| **L3** | Contribution | Fused CUDA Kernel | **O(1)** | æœ€çµ‚æˆæœ |

---

## ğŸ”¬ Rejection Sampling æ¼”ç®—æ³•è©³è§£

### æ•¸å­¸åŸç†ï¼ˆä¾†è‡ªè«–æ–‡ [1]ï¼‰

çµ¦å®šï¼š
- `p(x)` = draft model çš„æ©Ÿç‡åˆ†å¸ƒ
- `q(x)` = target model çš„æ©Ÿç‡åˆ†å¸ƒ
- `xÌ‚` = draft model æå‡ºçš„ token

**Accept/Reject è¦å‰‡ï¼š**
```
r ~ Uniform(0, 1)
if r < q(xÌ‚) / p(xÌ‚):
    ACCEPT xÌ‚
else:
    REJECT, resample from: q'(x) = norm(max(0, q(x) - p(x)))
```

### æ¼”ç®—æ³•æµç¨‹åœ–

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  é–‹å§‹é©—è­‰ K tokens â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  for k = 0 to K-1:       â”‚
              â”‚    token = draft[k]      â”‚
              â”‚    p = draft_prob[token] â”‚
              â”‚    q = target_prob[token]â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  r < q/p ?     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                    â–¼           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ ACCEPT  â”‚ â”‚   REJECT     â”‚
              â”‚ token   â”‚ â”‚ + Resample   â”‚
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚             â”‚
                   â–¼             â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ ç¹¼çºŒä¸‹ä¸€å€‹â”‚ â”‚  BREAK è¿´åœˆ  â”‚
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚             â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  è‹¥å…¨éƒ¨ ACCEPTï¼š        â”‚
              â”‚  å¾ target æ¡æ¨£ bonus   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
pp25_final_project/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ IMPLEMENTATION_GUIDE.md     # æœ¬æ–‡ä»¶
â”‚
â”œâ”€â”€ spec_decode/                     # vLLM åƒè€ƒå¯¦ä½œï¼ˆå”¯è®€ï¼‰
â”‚   â”œâ”€â”€ eagle.py                     # EAGLE proposer
â”‚   â”œâ”€â”€ medusa.py                    # Medusa proposer
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ baseline/                    # Level 1: Baseline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rejection_sampler.py     # PyTorch for loop å¯¦ä½œ
â”‚   â”‚
â”‚   â”œâ”€â”€ compiled/                    # Level 2: torch.compile
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ rejection_sampler.py     # @torch.compile ç‰ˆæœ¬
â”‚   â”‚
â”‚   â””â”€â”€ cuda/                        # Level 3: CUDA Kernel
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ fused_sampler.cu         # CUDA kernel å¯¦ä½œ
â”‚       â”œâ”€â”€ fused_sampler.cpp        # PyTorch C++ bindings
â”‚       â””â”€â”€ setup.py                 # ç·¨è­¯è…³æœ¬
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_correctness.py          # æ­£ç¢ºæ€§æ¸¬è©¦ï¼ˆGolden Standardï¼‰
â”‚   â””â”€â”€ conftest.py                  # pytest fixtures
â”‚
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ benchmark.py                 # æ•ˆèƒ½æ¸¬è©¦ä¸»ç¨‹å¼
â”‚   â”œâ”€â”€ plot_results.py              # ç¹ªè£½ "Money Slide" åœ–è¡¨
â”‚   â””â”€â”€ results/                     # æ¸¬è©¦çµæœè¼¸å‡º
â”‚
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ nsys_traces/                 # Nsight Systems åˆ†æçµæœ
â”‚   â””â”€â”€ compiler_analysis.md         # torch.compile å¤±æ•—åˆ†æ
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

---

## ğŸ“ å¯¦ä½œæ­¥é©Ÿè©³è§£

### ç¬¬ä¸€éšæ®µï¼šç’°å¢ƒè¨­ç½®èˆ‡ Baselineï¼ˆWeek 1-2ï¼‰

#### Step 1.1: ç’°å¢ƒé…ç½®

```bash
# å»ºç«‹è™›æ“¬ç’°å¢ƒ
python -m venv venv
source venv/bin/activate

# å®‰è£ä¾è³´
pip install torch numpy pytest matplotlib

# ç¢ºèª CUDA å¯ç”¨
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

#### Step 1.2: å¯¦ä½œ Level 1 Baseline

**æª”æ¡ˆ**: `src/baseline/rejection_sampler.py`

```python
"""
Level 1 Baseline: Naive PyTorch Implementation
==============================================
é€™æ˜¯æ­£ç¢ºæ€§çš„ã€Œé»ƒé‡‘æ¨™æº–ã€ï¼Œä½†æ•ˆèƒ½æœ€å·®ï¼ˆO(K) kernel launchesï¼‰
"""

import torch
from typing import Tuple

def rejection_sample_baseline(
    draft_probs: torch.Tensor,      # [batch_size, K, vocab_size]
    target_probs: torch.Tensor,     # [batch_size, K, vocab_size]
    draft_token_ids: torch.Tensor,  # [batch_size, K]
    bonus_probs: torch.Tensor,      # [batch_size, vocab_size] - ç”¨æ–¼ bonus token
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    åŸ·è¡Œ Rejection Sampling é©—è­‰ draft tokens
    
    Args:
        draft_probs: Draft model åœ¨æ¯å€‹ä½ç½®çš„æ©Ÿç‡åˆ†å¸ƒ
        target_probs: Target model åœ¨æ¯å€‹ä½ç½®çš„æ©Ÿç‡åˆ†å¸ƒ
        draft_token_ids: Draft model ç”¢ç”Ÿçš„ token IDs
        bonus_probs: ç”¨æ–¼æ¡æ¨£ bonus token çš„æ©Ÿç‡åˆ†å¸ƒ
    
    Returns:
        accepted_tokens: [batch_size, K+1] æ¥å—çš„ tokensï¼ˆå« bonusï¼‰
        num_accepted: [batch_size] æ¯å€‹ batch æ¥å—çš„ token æ•¸é‡
    """
    batch_size, K, vocab_size = draft_probs.shape
    device = draft_probs.device
    
    # è¼¸å‡º buffers
    accepted_tokens = torch.zeros(batch_size, K + 1, dtype=torch.long, device=device)
    num_accepted = torch.zeros(batch_size, dtype=torch.long, device=device)
    
    # å°æ¯å€‹ batch element ç¨ç«‹è™•ç†
    for b in range(batch_size):
        n_accepted = 0
        all_accepted = True
        
        # é©—è­‰ K å€‹ draft tokens
        for k in range(K):
            token_id = draft_token_ids[b, k].item()
            
            # å–å¾—è©² token çš„ draft/target æ©Ÿç‡
            p_draft = draft_probs[b, k, token_id].item()
            p_target = target_probs[b, k, token_id].item()
            
            # é¿å…é™¤ä»¥é›¶
            if p_draft < 1e-10:
                p_draft = 1e-10
            
            # Accept/Reject æ±ºç­–
            r = torch.rand(1, device=device).item()
            acceptance_prob = min(1.0, p_target / p_draft)
            
            if r < acceptance_prob:
                # ACCEPT: åŠ å…¥å·²æ¥å—åˆ—è¡¨
                accepted_tokens[b, n_accepted] = token_id
                n_accepted += 1
            else:
                # REJECT: å¾èª¿æ•´å¾Œçš„åˆ†å¸ƒ resample
                adjusted_probs = torch.clamp(
                    target_probs[b, k] - draft_probs[b, k], 
                    min=0.0
                )
                
                # æ­£è¦åŒ–
                prob_sum = adjusted_probs.sum()
                if prob_sum > 1e-10:
                    adjusted_probs = adjusted_probs / prob_sum
                else:
                    # Fallback: ä½¿ç”¨ target distribution
                    adjusted_probs = target_probs[b, k]
                
                # Resample
                resampled_token = torch.multinomial(adjusted_probs, 1).item()
                accepted_tokens[b, n_accepted] = resampled_token
                n_accepted += 1
                all_accepted = False
                break  # âš ï¸ EARLY EXIT - é€™æ˜¯é—œéµçš„å‹•æ…‹æ§åˆ¶æµï¼
        
        # è‹¥å…¨éƒ¨æ¥å—ï¼Œæ¡æ¨£ bonus token
        if all_accepted:
            bonus_token = torch.multinomial(bonus_probs[b], 1).item()
            accepted_tokens[b, n_accepted] = bonus_token
            n_accepted += 1
        
        num_accepted[b] = n_accepted
    
    return accepted_tokens, num_accepted
```

#### Step 1.3: å»ºç«‹æ¸¬è©¦å¥—ä»¶ï¼ˆGolden Standardï¼‰

**æª”æ¡ˆ**: `tests/test_correctness.py`

```python
"""
Correctness Tests (Golden Standard)
===================================
æ‰€æœ‰å¯¦ä½œéƒ½å¿…é ˆé€šéé€™äº›æ¸¬è©¦
"""

import pytest
import torch
from src.baseline.rejection_sampler import rejection_sample_baseline

# å›ºå®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§
SEED = 42

@pytest.fixture
def sample_data():
    """ç”Ÿæˆæ¸¬è©¦ç”¨çš„æ¨¡æ“¬è³‡æ–™"""
    torch.manual_seed(SEED)
    
    batch_size = 4
    K = 8
    vocab_size = 1000
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ç”Ÿæˆéš¨æ©Ÿ logits ä¸¦è½‰æ›ç‚ºæ©Ÿç‡
    draft_logits = torch.randn(batch_size, K, vocab_size, device=device)
    target_logits = torch.randn(batch_size, K, vocab_size, device=device)
    
    draft_probs = torch.softmax(draft_logits, dim=-1)
    target_probs = torch.softmax(target_logits, dim=-1)
    
    # å¾ draft distribution æ¡æ¨£ token ids
    draft_token_ids = torch.stack([
        torch.multinomial(draft_probs[:, k, :], 1).squeeze(-1)
        for k in range(K)
    ], dim=1)
    
    # Bonus probs
    bonus_probs = torch.softmax(
        torch.randn(batch_size, vocab_size, device=device), 
        dim=-1
    )
    
    return {
        "draft_probs": draft_probs,
        "target_probs": target_probs,
        "draft_token_ids": draft_token_ids,
        "bonus_probs": bonus_probs,
        "batch_size": batch_size,
        "K": K,
        "vocab_size": vocab_size,
    }


class TestBaseline:
    """æ¸¬è©¦ Level 1 Baseline"""
    
    def test_output_shape(self, sample_data):
        """ç¢ºèªè¼¸å‡ºå½¢ç‹€æ­£ç¢º"""
        result, num_accepted = rejection_sample_baseline(
            sample_data["draft_probs"],
            sample_data["target_probs"],
            sample_data["draft_token_ids"],
            sample_data["bonus_probs"],
        )
        
        batch_size = sample_data["batch_size"]
        K = sample_data["K"]
        
        assert result.shape == (batch_size, K + 1)
        assert num_accepted.shape == (batch_size,)
    
    def test_num_accepted_range(self, sample_data):
        """ç¢ºèªæ¥å—æ•¸é‡åœ¨åˆç†ç¯„åœå…§"""
        _, num_accepted = rejection_sample_baseline(
            sample_data["draft_probs"],
            sample_data["target_probs"],
            sample_data["draft_token_ids"],
            sample_data["bonus_probs"],
        )
        
        K = sample_data["K"]
        
        # è‡³å°‘æ¥å— 1 å€‹ï¼ˆreject å¾Œçš„ resampleï¼‰
        # æœ€å¤šæ¥å— K+1 å€‹ï¼ˆå…¨éƒ¨ accept + bonusï¼‰
        assert (num_accepted >= 1).all()
        assert (num_accepted <= K + 1).all()
    
    def test_accepted_tokens_valid(self, sample_data):
        """ç¢ºèªæ¥å—çš„ tokens éƒ½æ˜¯æœ‰æ•ˆçš„ vocab indices"""
        result, num_accepted = rejection_sample_baseline(
            sample_data["draft_probs"],
            sample_data["target_probs"],
            sample_data["draft_token_ids"],
            sample_data["bonus_probs"],
        )
        
        vocab_size = sample_data["vocab_size"]
        
        # åªæª¢æŸ¥å¯¦éš›æ¥å—çš„ tokens
        for b in range(sample_data["batch_size"]):
            n = num_accepted[b].item()
            valid_tokens = result[b, :n]
            assert (valid_tokens >= 0).all()
            assert (valid_tokens < vocab_size).all()
    
    def test_deterministic_with_seed(self, sample_data):
        """ç¢ºèªå›ºå®šç¨®å­æ™‚çµæœå¯é‡ç¾"""
        torch.manual_seed(SEED)
        result1, num1 = rejection_sample_baseline(
            sample_data["draft_probs"],
            sample_data["target_probs"],
            sample_data["draft_token_ids"],
            sample_data["bonus_probs"],
        )
        
        torch.manual_seed(SEED)
        result2, num2 = rejection_sample_baseline(
            sample_data["draft_probs"],
            sample_data["target_probs"],
            sample_data["draft_token_ids"],
            sample_data["bonus_probs"],
        )
        
        assert torch.equal(num1, num2)


class TestCompareImplementations:
    """æ¯”è¼ƒä¸åŒå¯¦ä½œçš„æ­£ç¢ºæ€§"""
    
    def test_compiled_matches_baseline(self, sample_data):
        """Level 2 æ‡‰èˆ‡ Level 1 çµæœä¸€è‡´ï¼ˆçµ±è¨ˆä¸Šï¼‰"""
        # TODO: å¯¦ä½œ Level 2 å¾Œå•Ÿç”¨
        pass
    
    def test_cuda_matches_baseline(self, sample_data):
        """Level 3 æ‡‰èˆ‡ Level 1 çµæœä¸€è‡´ï¼ˆçµ±è¨ˆä¸Šï¼‰"""
        # TODO: å¯¦ä½œ Level 3 å¾Œå•Ÿç”¨
        pass
```

---

### ç¬¬äºŒéšæ®µï¼štorch.compile åˆ†æï¼ˆWeek 2ï¼‰

#### Step 2.1: å¯¦ä½œ Level 2

**æª”æ¡ˆ**: `src/compiled/rejection_sampler.py`

```python
"""
Level 2: torch.compile Version
==============================
å±•ç¤º SOTA ç·¨è­¯å™¨åœ¨å‹•æ…‹æ§åˆ¶æµä¸Šçš„é™åˆ¶
"""

import torch
from typing import Tuple

@torch.compile(mode="reduce-overhead")
def rejection_sample_compiled(
    draft_probs: torch.Tensor,
    target_probs: torch.Tensor,
    draft_token_ids: torch.Tensor,
    bonus_probs: torch.Tensor,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    èˆ‡ baseline ç›¸åŒé‚è¼¯ï¼Œä½†åŠ ä¸Š @torch.compile
    
    é æœŸçµæœï¼štorch.compile ç„¡æ³•æœ‰æ•ˆèåˆé€™å€‹å‡½æ•¸ï¼Œ
    å› ç‚ºå­˜åœ¨ data-dependent çš„ break æ§åˆ¶æµ
    """
    batch_size, K, vocab_size = draft_probs.shape
    device = draft_probs.device
    
    accepted_tokens = torch.zeros(batch_size, K + 1, dtype=torch.long, device=device)
    num_accepted = torch.zeros(batch_size, dtype=torch.long, device=device)
    
    for b in range(batch_size):
        n_accepted = 0
        all_accepted = True
        
        for k in range(K):
            token_id = draft_token_ids[b, k]
            p_draft = draft_probs[b, k, token_id]
            p_target = target_probs[b, k, token_id]
            
            p_draft = torch.clamp(p_draft, min=1e-10)
            r = torch.rand(1, device=device)
            acceptance_prob = torch.minimum(
                torch.ones(1, device=device), 
                p_target / p_draft
            )
            
            if r < acceptance_prob:
                accepted_tokens[b, n_accepted] = token_id
                n_accepted += 1
            else:
                adjusted_probs = torch.clamp(
                    target_probs[b, k] - draft_probs[b, k], 
                    min=0.0
                )
                prob_sum = adjusted_probs.sum()
                adjusted_probs = torch.where(
                    prob_sum > 1e-10,
                    adjusted_probs / prob_sum,
                    target_probs[b, k]
                )
                resampled_token = torch.multinomial(adjusted_probs, 1)
                accepted_tokens[b, n_accepted] = resampled_token.squeeze()
                n_accepted += 1
                all_accepted = False
                break  # âš ï¸ é€™å€‹ break æœƒå°è‡´ graph break!
        
        if all_accepted:
            bonus_token = torch.multinomial(bonus_probs[b], 1)
            accepted_tokens[b, n_accepted] = bonus_token.squeeze()
            n_accepted += 1
        
        num_accepted[b] = n_accepted
    
    return accepted_tokens, num_accepted
```

#### Step 2.2: åˆ†æ torch.compile çš„é™åˆ¶

**æª”æ¡ˆ**: `analysis/compiler_analysis.md`

```markdown
# torch.compile ç·¨è­¯å™¨åˆ†æ

## å•é¡Œï¼šGraph Breaks

ç•¶ä½¿ç”¨ `TORCH_LOGS="graph_breaks" python benchmark.py` åŸ·è¡Œæ™‚ï¼Œ
æœƒçœ‹åˆ°é¡ä¼¼ä»¥ä¸‹çš„è¼¸å‡ºï¼š

```
[graph_break] Dynamic control flow: data-dependent break statement
  File "rejection_sampler.py", line XX
    break  # âš ï¸ é€™å€‹ break æœƒå°è‡´ graph break!
```

## ç‚ºä»€éº¼ torch.compile ç„¡æ³•è™•ç†ï¼Ÿ

1. **Data-dependent Control Flow**: 
   - `if r < acceptance_prob` çš„çµæœåœ¨ç·¨è­¯æ™‚æœªçŸ¥
   - `break` èªå¥æœƒæ ¹æ“šé‹è¡Œæ™‚æ•¸æ“šæå‰é€€å‡º

2. **Graph åˆ†è£‚**:
   - æ¯æ¬¡ `break` éƒ½æœƒç”¢ç”Ÿä¸€å€‹æ–°çš„ graph
   - å°è‡´å¯¦éš›ä¸Šä»ç„¶æ˜¯ O(K) æ¬¡ kernel launch

3. **ç„¡æ³•å‘é‡åŒ–**:
   - ä¸åŒ batch elements å¯èƒ½åœ¨ä¸åŒä½ç½® reject
   - ç„¡æ³•ç°¡å–®åœ°ç”¨ SIMD è™•ç†

## çµè«–

æ‰‹å‹• CUDA kernel æ˜¯å¿…è¦çš„ï¼Œå› ç‚ºï¼š
- å¯ä»¥åœ¨å–®ä¸€ kernel å…§è™•ç†æ‰€æœ‰å‹•æ…‹é‚è¼¯
- æ¯å€‹ thread ç¨ç«‹è™•ç†ä¸€å€‹ batch element
- ä½¿ç”¨ on-device RNG é¿å… CPU å¾€è¿”
```

---

### ç¬¬ä¸‰éšæ®µï¼šCUDA Kernel å¯¦ä½œï¼ˆWeek 3-4ï¼‰

#### Step 3.1: CUDA Kernel è¨­è¨ˆ

**æª”æ¡ˆ**: `src/cuda/fused_sampler.cu`

```cpp
/*
 * Level 3: Fused CUDA Kernel for Rejection Sampling
 * ==================================================
 * 
 * è¨­è¨ˆç›®æ¨™ï¼š
 * 1. å–®ä¸€ kernel launch è™•ç†æ•´å€‹ batch
 * 2. æ¯å€‹ thread è™•ç†ä¸€å€‹ batch element
 * 3. On-device RNG (curand)
 * 4. æ­£ç¢ºè™•ç† variable-length output
 */

#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <curand_kernel.h>

// CUDA éŒ¯èª¤æª¢æŸ¥ macro
#define CUDA_CHECK(call) \
    do { \
        cudaError_t err = call; \
        if (err != cudaSuccess) { \
            fprintf(stderr, "CUDA error at %s:%d: %s\n", \
                    __FILE__, __LINE__, cudaGetErrorString(err)); \
            exit(EXIT_FAILURE); \
        } \
    } while(0)

// åˆå§‹åŒ– RNG states
__global__ void init_rng_kernel(
    curandState* states,
    unsigned long long seed,
    int num_states
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < num_states) {
        curand_init(seed, idx, 0, &states[idx]);
    }
}

// ä¸»è¦çš„ Fused Rejection Sampling Kernel
__global__ void fused_rejection_sample_kernel(
    const float* __restrict__ draft_probs,      // [batch, K, vocab]
    const float* __restrict__ target_probs,     // [batch, K, vocab]
    const int64_t* __restrict__ draft_token_ids, // [batch, K]
    const float* __restrict__ bonus_probs,      // [batch, vocab]
    int64_t* __restrict__ accepted_tokens,      // [batch, K+1] output
    int64_t* __restrict__ num_accepted,         // [batch] output
    curandState* __restrict__ rng_states,
    const int batch_size,
    const int K,
    const int vocab_size
) {
    // æ¯å€‹ thread è™•ç†ä¸€å€‹ batch element
    const int batch_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (batch_idx >= batch_size) return;
    
    // è¼‰å…¥ local RNG state
    curandState local_rng = rng_states[batch_idx];
    
    // è¨ˆç®— base offsets
    const int prob_batch_offset = batch_idx * K * vocab_size;
    const int token_batch_offset = batch_idx * K;
    const int output_batch_offset = batch_idx * (K + 1);
    const int bonus_batch_offset = batch_idx * vocab_size;
    
    int n_accepted = 0;
    bool all_accepted = true;
    
    // ============================================
    // æ ¸å¿ƒé‚è¼¯ï¼šé©—è­‰ K å€‹ draft tokens
    // ============================================
    for (int k = 0; k < K; k++) {
        const int token_id = static_cast<int>(draft_token_ids[token_batch_offset + k]);
        
        // å–å¾—è©² token çš„æ©Ÿç‡
        const int prob_offset = prob_batch_offset + k * vocab_size + token_id;
        float p_draft = draft_probs[prob_offset];
        float p_target = target_probs[prob_offset];
        
        // é¿å…é™¤ä»¥é›¶
        p_draft = fmaxf(p_draft, 1e-10f);
        
        // Accept/Reject æ±ºç­–
        float r = curand_uniform(&local_rng);
        float acceptance_prob = fminf(1.0f, p_target / p_draft);
        
        if (r < acceptance_prob) {
            // ACCEPT
            accepted_tokens[output_batch_offset + n_accepted] = token_id;
            n_accepted++;
        } else {
            // REJECT: Resample from adjusted distribution
            // è¨ˆç®— adjusted_probs = max(0, target - draft)
            
            const int k_prob_offset = prob_batch_offset + k * vocab_size;
            float prob_sum = 0.0f;
            
            // ç¬¬ä¸€éï¼šè¨ˆç®— sum
            for (int v = 0; v < vocab_size; v++) {
                float adj = fmaxf(0.0f, target_probs[k_prob_offset + v] 
                                      - draft_probs[k_prob_offset + v]);
                prob_sum += adj;
            }
            
            // Multinomial sampling from adjusted distribution
            float u = curand_uniform(&local_rng) * prob_sum;
            float cumsum = 0.0f;
            int resampled_token = 0;
            
            for (int v = 0; v < vocab_size; v++) {
                float adj = fmaxf(0.0f, target_probs[k_prob_offset + v] 
                                      - draft_probs[k_prob_offset + v]);
                cumsum += adj;
                if (cumsum >= u) {
                    resampled_token = v;
                    break;
                }
            }
            
            accepted_tokens[output_batch_offset + n_accepted] = resampled_token;
            n_accepted++;
            all_accepted = false;
            break;  // EARLY EXIT - åœ¨ GPU å…§è‡ªç„¶è™•ç†ï¼
        }
    }
    
    // ============================================
    // è‹¥å…¨éƒ¨ acceptï¼Œæ¡æ¨£ bonus token
    // ============================================
    if (all_accepted) {
        // Multinomial sampling from bonus_probs
        float u = curand_uniform(&local_rng);
        float cumsum = 0.0f;
        int bonus_token = 0;
        
        for (int v = 0; v < vocab_size; v++) {
            cumsum += bonus_probs[bonus_batch_offset + v];
            if (cumsum >= u) {
                bonus_token = v;
                break;
            }
        }
        
        accepted_tokens[output_batch_offset + n_accepted] = bonus_token;
        n_accepted++;
    }
    
    // å„²å­˜æ¥å—æ•¸é‡
    num_accepted[batch_idx] = n_accepted;
    
    // å„²å­˜ RNG state
    rng_states[batch_idx] = local_rng;
}


// ============================================
// PyTorch C++ Extension Interface
// ============================================

class FusedRejectionSampler {
public:
    FusedRejectionSampler(int max_batch_size, unsigned long long seed = 42) 
        : max_batch_size_(max_batch_size), initialized_(false) {
        
        // åˆ†é… RNG states
        CUDA_CHECK(cudaMalloc(&rng_states_, max_batch_size * sizeof(curandState)));
        
        // åˆå§‹åŒ– RNG
        int threads = 256;
        int blocks = (max_batch_size + threads - 1) / threads;
        init_rng_kernel<<<blocks, threads>>>(rng_states_, seed, max_batch_size);
        CUDA_CHECK(cudaGetLastError());
        
        initialized_ = true;
    }
    
    ~FusedRejectionSampler() {
        if (initialized_) {
            cudaFree(rng_states_);
        }
    }
    
    std::tuple<torch::Tensor, torch::Tensor> sample(
        torch::Tensor draft_probs,
        torch::Tensor target_probs,
        torch::Tensor draft_token_ids,
        torch::Tensor bonus_probs
    ) {
        const int batch_size = draft_probs.size(0);
        const int K = draft_probs.size(1);
        const int vocab_size = draft_probs.size(2);
        
        // ç¢ºä¿è¼¸å…¥åœ¨ GPU ä¸Š
        TORCH_CHECK(draft_probs.is_cuda(), "draft_probs must be on CUDA");
        TORCH_CHECK(target_probs.is_cuda(), "target_probs must be on CUDA");
        TORCH_CHECK(draft_token_ids.is_cuda(), "draft_token_ids must be on CUDA");
        TORCH_CHECK(bonus_probs.is_cuda(), "bonus_probs must be on CUDA");
        
        // ç¢ºä¿ contiguous
        draft_probs = draft_probs.contiguous();
        target_probs = target_probs.contiguous();
        draft_token_ids = draft_token_ids.contiguous();
        bonus_probs = bonus_probs.contiguous();
        
        // åˆ†é…è¼¸å‡º tensors
        auto options_long = torch::TensorOptions()
            .dtype(torch::kInt64)
            .device(draft_probs.device());
        
        torch::Tensor accepted_tokens = torch::zeros({batch_size, K + 1}, options_long);
        torch::Tensor num_accepted = torch::zeros({batch_size}, options_long);
        
        // Launch kernel
        int threads = 256;
        int blocks = (batch_size + threads - 1) / threads;
        
        fused_rejection_sample_kernel<<<blocks, threads>>>(
            draft_probs.data_ptr<float>(),
            target_probs.data_ptr<float>(),
            draft_token_ids.data_ptr<int64_t>(),
            bonus_probs.data_ptr<float>(),
            accepted_tokens.data_ptr<int64_t>(),
            num_accepted.data_ptr<int64_t>(),
            rng_states_,
            batch_size,
            K,
            vocab_size
        );
        
        CUDA_CHECK(cudaGetLastError());
        
        return std::make_tuple(accepted_tokens, num_accepted);
    }

private:
    int max_batch_size_;
    curandState* rng_states_;
    bool initialized_;
};


// Python bindings
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    pybind11::class_<FusedRejectionSampler>(m, "FusedRejectionSampler")
        .def(pybind11::init<int, unsigned long long>(),
             pybind11::arg("max_batch_size"),
             pybind11::arg("seed") = 42)
        .def("sample", &FusedRejectionSampler::sample);
}
```

#### Step 3.2: ç·¨è­¯è…³æœ¬

**æª”æ¡ˆ**: `src/cuda/setup.py`

```python
from setuptools import setup
from torch.utils.cpp_extension import BuildExtension, CUDAExtension

setup(
    name='fused_sampler',
    ext_modules=[
        CUDAExtension(
            name='fused_sampler',
            sources=['fused_sampler.cu'],
            extra_compile_args={
                'cxx': ['-O3'],
                'nvcc': ['-O3', '--use_fast_math']
            }
        )
    ],
    cmdclass={
        'build_ext': BuildExtension
    }
)
```

ç·¨è­¯æŒ‡ä»¤ï¼š
```bash
cd src/cuda
python setup.py install
```

---

### ç¬¬å››éšæ®µï¼šæ•ˆèƒ½æ¸¬è©¦èˆ‡åˆ†æï¼ˆWeek 5-6ï¼‰

#### Step 4.1: Benchmark è…³æœ¬

**æª”æ¡ˆ**: `benchmarks/benchmark.py`

```python
"""
Performance Benchmark
=====================
æ¯”è¼ƒä¸‰å€‹å¯¦ä½œå±¤ç´šçš„æ•ˆèƒ½
"""

import torch
import time
import json
import argparse
from pathlib import Path

# Import implementations
from src.baseline.rejection_sampler import rejection_sample_baseline
from src.compiled.rejection_sampler import rejection_sample_compiled
import fused_sampler  # CUDA extension


def generate_test_data(batch_size: int, K: int, vocab_size: int, device: str):
    """ç”Ÿæˆæ¸¬è©¦è³‡æ–™"""
    draft_logits = torch.randn(batch_size, K, vocab_size, device=device)
    target_logits = torch.randn(batch_size, K, vocab_size, device=device)
    
    draft_probs = torch.softmax(draft_logits, dim=-1)
    target_probs = torch.softmax(target_logits, dim=-1)
    
    draft_token_ids = torch.stack([
        torch.multinomial(draft_probs[:, k, :], 1).squeeze(-1)
        for k in range(K)
    ], dim=1)
    
    bonus_probs = torch.softmax(
        torch.randn(batch_size, vocab_size, device=device),
        dim=-1
    )
    
    return draft_probs, target_probs, draft_token_ids, bonus_probs


def benchmark_function(fn, args, warmup: int = 10, iterations: int = 100):
    """æ¸¬é‡å‡½æ•¸åŸ·è¡Œæ™‚é–“"""
    # Warmup
    for _ in range(warmup):
        fn(*args)
    torch.cuda.synchronize()
    
    # Benchmark
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    
    start_event.record()
    for _ in range(iterations):
        fn(*args)
    end_event.record()
    torch.cuda.synchronize()
    
    elapsed_ms = start_event.elapsed_time(end_event) / iterations
    return elapsed_ms * 1000  # Convert to Âµs


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch-size", type=int, default=32)
    parser.add_argument("--vocab-size", type=int, default=32000)
    parser.add_argument("--iterations", type=int, default=100)
    parser.add_argument("--output", type=str, default="results/benchmark.json")
    args = parser.parse_args()
    
    device = "cuda"
    K_values = [2, 4, 8, 16]
    
    results = {
        "config": {
            "batch_size": args.batch_size,
            "vocab_size": args.vocab_size,
            "iterations": args.iterations,
        },
        "data": []
    }
    
    # åˆå§‹åŒ– CUDA sampler
    cuda_sampler = fused_sampler.FusedRejectionSampler(args.batch_size)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š SPECULATIVE DECODING REJECTION SAMPLER BENCHMARK")
    print("=" * 70)
    print(f"Configuration: batch_size={args.batch_size}, vocab_size={args.vocab_size}")
    print("-" * 70)
    print(f"{'K':<6} {'L1 Baseline (Âµs)':<20} {'L2 Compile (Âµs)':<20} {'L3 CUDA (Âµs)':<20}")
    print("-" * 70)
    
    for K in K_values:
        # ç”Ÿæˆæ¸¬è©¦è³‡æ–™
        data = generate_test_data(args.batch_size, K, args.vocab_size, device)
        
        # Level 1: Baseline
        t1 = benchmark_function(
            rejection_sample_baseline, 
            data, 
            iterations=args.iterations
        )
        
        # Level 2: torch.compile
        t2 = benchmark_function(
            rejection_sample_compiled,
            data,
            iterations=args.iterations
        )
        
        # Level 3: CUDA Kernel
        t3 = benchmark_function(
            cuda_sampler.sample,
            data,
            iterations=args.iterations
        )
        
        print(f"{K:<6} {t1:<20.2f} {t2:<20.2f} {t3:<20.2f}")
        
        results["data"].append({
            "K": K,
            "L1_baseline_us": t1,
            "L2_compile_us": t2,
            "L3_cuda_us": t3,
            "speedup_vs_baseline": t1 / t3,
            "speedup_vs_compile": t2 / t3,
        })
    
    print("-" * 70)
    
    # å„²å­˜çµæœ
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… Results saved to {args.output}")
    
    # é¡¯ç¤º speedup æ‘˜è¦
    print("\nğŸ“ˆ SPEEDUP SUMMARY:")
    print("-" * 40)
    for entry in results["data"]:
        print(f"K={entry['K']}: "
              f"{entry['speedup_vs_baseline']:.1f}x vs baseline, "
              f"{entry['speedup_vs_compile']:.1f}x vs compile")


if __name__ == "__main__":
    main()
```

#### Step 4.2: ç¹ªè£½çµæœåœ–è¡¨ï¼ˆMoney Slideï¼‰

**æª”æ¡ˆ**: `benchmarks/plot_results.py`

```python
"""
Generate "Money Slide" Performance Graph
========================================
"""

import json
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path


def plot_results(results_path: str, output_path: str = "results/performance.png"):
    with open(results_path) as f:
        results = json.load(f)
    
    data = results["data"]
    K_values = [d["K"] for d in data]
    baseline = [d["L1_baseline_us"] for d in data]
    compiled = [d["L2_compile_us"] for d in data]
    cuda = [d["L3_cuda_us"] for d in data]
    
    plt.figure(figsize=(10, 6))
    
    plt.plot(K_values, baseline, 'o-', linewidth=2, markersize=8, 
             label='L1: PyTorch Baseline', color='#e74c3c')
    plt.plot(K_values, compiled, 's-', linewidth=2, markersize=8,
             label='L2: torch.compile', color='#f39c12')
    plt.plot(K_values, cuda, '^-', linewidth=2, markersize=8,
             label='L3: Fused CUDA Kernel', color='#27ae60')
    
    plt.xlabel('K (Number of Draft Tokens)', fontsize=12)
    plt.ylabel('Latency (Âµs)', fontsize=12)
    plt.title('Rejection Sampling Performance: O(K) vs O(1)', fontsize=14)
    plt.legend(loc='upper left', fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.xticks(K_values)
    
    # æ¨™è¨» speedup
    for i, k in enumerate(K_values):
        speedup = baseline[i] / cuda[i]
        plt.annotate(f'{speedup:.1f}x', 
                    xy=(k, cuda[i]), 
                    xytext=(k + 0.3, cuda[i] + 50),
                    fontsize=9, color='#27ae60')
    
    plt.tight_layout()
    
    output = Path(output_path)
    output.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(output, dpi=150)
    print(f"âœ… Plot saved to {output}")


if __name__ == "__main__":
    plot_results("results/benchmark.json")
```

---

## âœ… æª¢æŸ¥æ¸…å–®

### Week 1-2
- [ ] ç’°å¢ƒè¨­ç½®å®Œæˆ
- [ ] Level 1 Baseline å¯¦ä½œå®Œæˆ
- [ ] Unit tests (Golden Standard) é€šé
- [ ] Level 2 torch.compile ç‰ˆæœ¬å¯¦ä½œ
- [ ] ç¢ºèª torch.compile æœ‰ graph breaks

### Week 3-4
- [ ] CUDA kernel æ¶æ§‹è¨­è¨ˆ
- [ ] curand RNG æ•´åˆ
- [ ] Kernel ç·¨è­¯æˆåŠŸ
- [ ] é€šé correctness tests
- [ ] è™•ç† edge cases

### Week 5-6
- [ ] Benchmark è…³æœ¬å®Œæˆ
- [ ] æ•ˆèƒ½æ•¸æ“šæ”¶é›†
- [ ] Money Slide åœ–è¡¨ç”¢ç”Ÿ
- [ ] nsys trace åˆ†æ
- [ ] å ±å‘Šæ’°å¯«å®Œæˆ

---

## ğŸ“š åƒè€ƒè³‡æº

1. **è«–æ–‡**: Leviathan et al., "Fast Inference from Transformers via Speculative Decoding" (ICML 2023)
2. **vLLM åŸå§‹ç¢¼**: `spec_decode/` è³‡æ–™å¤¾
3. **CUDA Programming Guide**: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
4. **PyTorch C++ Extension**: https://pytorch.org/tutorials/advanced/cpp_extension.html
5. **cuRAND Library**: https://docs.nvidia.com/cuda/curand/

---

## ğŸ¤” å¸¸è¦‹å•é¡Œ

### Q: ç‚ºä»€éº¼ä¸èƒ½ç”¨ `torch.compile` è§£æ±ºé€™å€‹å•é¡Œï¼Ÿ
A: `torch.compile` ç„¡æ³•è™•ç† data-dependent control flowï¼ˆå¦‚ `break`ï¼‰ã€‚ç•¶é‡åˆ°é€™ç¨®æƒ…æ³æ™‚ï¼Œå®ƒæœƒç”¢ç”Ÿ "graph break"ï¼Œå°è‡´ä»ç„¶æ˜¯ O(K) æ¬¡ kernel launchã€‚

### Q: CUDA kernel å¦‚ä½•è™•ç† variable-length outputï¼Ÿ
A: æ¯å€‹ thread ä½¿ç”¨è‡ªå·±çš„ counter (`n_accepted`)ï¼Œæœ€å¾Œå¯«å…¥ `num_accepted[batch_idx]`ã€‚å‘¼å«è€…æ ¹æ“šé€™å€‹å€¼çŸ¥é“æ¯å€‹ batch element å¯¦éš›ç”¢ç”Ÿäº†å¤šå°‘ tokensã€‚

### Q: å¦‚ä½•ç¢ºä¿ correctnessï¼Ÿ
A: ä½¿ç”¨å›ºå®šéš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿ baseline å’Œ CUDA kernel åœ¨çµ±è¨ˆä¸Šç”¢ç”Ÿç›¸åŒåˆ†å¸ƒçš„è¼¸å‡ºã€‚å…·é«”é€šé chi-squared test æˆ– KL divergence é©—è­‰ã€‚

---

*æœ€å¾Œæ›´æ–°: 2025å¹´11æœˆ*
